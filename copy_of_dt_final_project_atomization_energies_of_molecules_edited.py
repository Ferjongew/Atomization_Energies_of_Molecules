# -*- coding: utf-8 -*-
"""Copy of DT_Final_Project_Atomization_Energies_of_Molecules_Edited.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lMGH-ErxR273sLTziZo_2usGD-oclt6h
"""

import numpy as np
import pandas as pd
import math
import seaborn as sns

#Plotting
import seaborn as sns #; sns.set()
from matplotlib import style
from matplotlib import pyplot
from matplotlib import pyplot as plt



import seaborn as sns
import matplotlib.pyplot as plt
#from matplotlib import style
from matplotlib import pyplot
from matplotlib import pyplot as plt

from mpl_toolkits.mplot3d import Axes3D


#from sklearn.cross_validation import train_test_split
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,  normalize # standarizes (pre-process) the data by substracting the mean and dividing it by the std giving a norm =1 and mean=0.

# Chapter Two
from scipy import linalg as LA
from scipy.stats import randint as sp_randint
from sklearn.decomposition import PCA

from sklearn.metrics import mean_squared_error


#plt.rcParams['figure.figsize'] = [20, 15]

from IPython.display import Image
from sklearn.model_selection import GridSearchCV

from google.colab import files
uploaded = files.upload()

from google.colab import drive
drive.mount('/content/gdrive')

#Reading the data
MoleQM = pd.read_csv('/content/gdrive/MyDrive/roboBohr.csv')

MoleQM.shape

#The data is composed of 16242 rows, each representing a molecule and 1278 columns whch we need to look at.
## 1275 (upon removal of unwanted columns) when data is represented by CIJ (full Coulumb matrices)

MoleQM.columns

"""##Columns in the dataset

*   **Unnamed**: 0 - Index for each molecule.

*   **0-1274** - 1275 entries in the Coulomb matrix that act as molecular features. For example: Row=0, Column = 0 is the first molecule, and the first entry of the Coulomb matrix describing it.

*  **pubchem_id** - Pubchem Id where the molecular structures are obtained. A unique identifier for each molecule.

*  **Eat** - atomization energy calculated by simulations using the Quantum Espresso package.

"""

MoleQM.head()

# We can see that there are two columns that will need to be dropped since we have no use for them.

# Dropping: Unnamed:0 and pubchem_id.

MoleQM = MoleQM.drop(['Unnamed: 0', 'pubchem_id'], axis = 1)
MoleQM.head()

# Checking for posible missing values
MoleQM.isnull().sum().sum()

# Missing values can be a source of errors in the code, so I make sure There arent any.

MoleQM.shape

"""## Atomization Energy Distribution"""

# Looking at the Target Variable: Eat

MoleQM.Eat.describe()

from scipy.stats import norm
#ax = sns.distplot(x, fit=norm, kde=False)
sns.distplot(MoleQM['Eat'], fit=norm, hist=True,  kde=True, color="g")
plt.xlabel('Atomization Energy (kcal/mol)')
plt.ylabel ('Density')
plt.title('Atomization Energy Distribution')

# Before applying. PCA, I first separate the featres from the target variabes.
MoleQM_Y = MoleQM['Eat'].values

# Separating out the target
MoleQM_X = MoleQM.drop(['Eat'],axis=1).values # values: Return a Numpy representation of the DataFrame.

MoleQM_Y.shape
MoleQM_X.shape

#dependent variable Eat.
MoleQM_Y[0:10]

"""### Standardizing

"""

# Before applying PCA, I normalize the data to have mean 0 along each dimension and variance 1.


MoleQM_X_fit = StandardScaler().fit_transform(MoleQM_X)
MoleQM = MoleQM['Eat']

# The transformation works by subtracting each entry by the mean, so
# any entries that are less than the average value of all entries will become negative.
# Hence, this explains the negative values below:
#PCA calculates a new projection of our data on a new axis using the standard deviation of our data

MoleQM_X_fit

MoleQM_X_fit.shape

"""##PCA"""

# PCA will give me a smaller set of features that describe the overall trends. It helps us combine correlated variables to reduce the features.
#Main task in this PCA is to select a subset of variables from a larger set, based on which original variables have the highest correlation with the principal amount.
pca = PCA(.85)
MoleQM_X_PCA = pca.fit_transform(MoleQM_X_fit)
PCA_score = pca.explained_variance_ratio_
Var = pca.components_

pca.n_components_

#Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.

print(pca.singular_values_)

#The singular values corresponding to each of the selected components.
#The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space.

# Visualizing PCA components
#I check that the inputs are reasonably distributed by plotting a two-dimensional PCA of data.

fig = plt.figure(figsize=(16, 6))
ax1 = fig.add_subplot(111)
ax1.set_ylim([0,1])
lin1 = ax1.scatter(range(0, int(PCA_score.shape[0])), PCA_score, c = 'b', label = 'no random noise')

plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Components Pattern')

plt.show()

# This plot shows that about 80% of the variance can be mainly explained by three principal components.

MoleQM_X_PCA.shape

print(pca.explained_variance_ratio_)

#Percentage of variance explained by each of the selected components.
#By using the attribute explained_variance_ratio_, you can see that the first principal component contains 62.15% of the variance
#and the second principal component contains 25.78% of the variance. Together, the two components contain 89% of the information.

# percentage of the variance explained by each features.
#We can compare the variance in the overall dataset to what was captured from our two primary components using .explained_variance_ratio_.
#The principal components are constructed from the eigenvectors of the covariance matrix, and are ordered with increasing eigenvalues. For instance, the first principal
#component corresponds to the linear combination of columns of X with the largest eigenvalue, hence has the largest variance

# Calculate the variance explained by priciple components
print('Variance of each component:', pca.explained_variance_ratio_)
print('\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))

#We can see that our first two principal components explain the majority of the variance in this dataset (86.05%)!
#This is an indication of the total information represented compared to the original data.

print(pca.explained_variance_)

#The amount of variance explained by each of the selected components.

#The explained variance tells you how much information (variance) can be attributed to each of the principal components

#This shows the variance of each (1275) features in the dataset (how much each component explain the features)

"""#### Plot of the two principal components againts one another. The dots are colored in proportion to their energy value."""

fig = plt.figure(figsize=(16, 6))
axis2 = fig.add_subplot(111)
axis2.scatter(MoleQM_X_PCA[:,0], MoleQM_X_PCA[:,1], c=MoleQM_Y)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Component Space')

plt.show()

#Plot of the atomization energies in the data as a function of the first two principal component vectors.
# This plot shows the peculiar nonlinear relationship between the feature vectors and the atomization energies.

#The graph shows the correlations between the original variables and the PCs.  For example, the correlations between an original variable and two PCs.
#The correlations with the first PC are plotted on the horizontal axis; the correlations with the second PC are plotted on the vertical axis.

#The dots are colored proportional to their energy values.

# For l, we want a non-linear map between molecules characteristics and atomization energies. For that, we need to look at the molecular similarities. PCA helps with. that

from mpl_toolkits.axes_grid1.inset_locator import mark_inset
fig = plt.figure(figsize=(16, 6))

#x = MoleQM_X_PCA
#y = MoleQM_Y

x = MoleQM_X_PCA[:,0]
y = MoleQM_X_PCA[:,1]

ax = plt.subplot(111)
ax.plot(x, y)
ax.set_ylim(-50, 50)

axis2.scatter(MoleQM_X_PCA[:,0], MoleQM_X_PCA[:,1], c=MoleQM_Y)
ax2 = plt.axes([0.2, 0.2, .6, .2])
ax2.plot(x, y)

#mark_inset(ax, ax2, loc1=2, loc2=1, fc="none", ec="0.5")

#mark_inset(parent_axes, inset_axes, loc1, loc2, **kwargs)

#axins = inset_axes(ax, 1,1 , loc=2,bbox_to_anchor=(0.2, 0.55),bbox_transform=ax.figure.transFigure)

#The 1,1 section sets the width and height of the inset, respectively.
#After this, use the xlim() and ylim() to set the extents of the axes without changing the size or shape of the inset box.

plt.show()


#ax = plt.axes(projection='3d')

# Data for a three-dimensional line
#zline = MoleQM_Y
#xline = MoleQM_X_PCA[:,0]
#yline = MoleQM_X_PCA[:,1]
#ax.scatter3D(xline, yline, zline, 'red')

ax = plt.axes(projection='3d')

# Data for a three-dimensional line
zline = MoleQM_Y
xline = MoleQM_X_PCA[:,0]
yline = MoleQM_X_PCA[:,1]
ax.scatter3D(xline, yline, zline, 'green')

# The plot shows a particular pattern for a. range of energies
# explain why these particular molecules (atoms) have similar energies range?

# Also, you should try to explain spike along the z-axis.  What is going on with
# the physics that is creating this pattern?

# We will try to figure out what makes the spike datapoints unique by separating
# them from the rest of the PCA data, identifying their pubchem indices, and
# looking back into the pubchem database to see the identities of some of these
# molecules.  There will be many thousands of molecules, so we can only feasibly
# identify a few of them.

# First, we need to reattach the pubchem id column to the pca data.

MoleQM_ID = MoleQM['pubchem_id'].values

MoleQM_ID_reshaped = np.reshape(MoleQM_ID, (16242, 1))

# Now, attach this column of ids to the PCA data.

MoleQM_X_PCA_ID = np.concatenate((MoleQM_X_PCA, MoleQM_ID_reshaped), axis = 1)

# Now, we need to select all of the MoleQM_X_PCA_ID rows corresponding to a
# range of values over the region of interest.  By inspecting the above graph,
# we choose this region to be PC1 in [-30, 30], PC2 in [-50, 50].

PCA_Data_of_Interest = MoleQM_X_PCA_ID[((MoleQM_X_PCA_ID >= [-50, -50, -10000000, -10000000]) & (MoleQM_X_PCA_ID <=[-15, 50, 10000000, 10000000])).all(1)]

# We can adjust the range by simply changing the first two entries in each of
# the square brackets above.  I will try to shorten the range to limit the
# number of molecules selected, by changing the range for PC1 to [-30, 0].

# Now I will try a third range to reduce it even more.
# Try PC1 in [-50, -15]

# Let's see what the dimensions of this new object are.

PCA_Data_of_Interest.shape

# The first range gives us 15,047 molecules around the spike.
# The second range gives us 12,212 molecules.
# The third range gives us 396 molecules.

#?????????

MoleQM_X_new = pca.inverse_transform(MoleQM_X_PCA)
plt.scatter(MoleQM_X[:, 0], MoleQM_X[:, 1], edgecolors='green', s=50,  alpha=0.4) # original data without pca
plt.scatter(MoleQM_X_new[:, 0], MoleQM_X_new[:, 1], alpha=0.8)
plt.axis()

#plt.axis('equal'); This instead gives me a smaller range

plt.colorbar()

# To understand the effect of the PCA reduction, I performed an inverse transform of the reduced data and plot it along with the original data.
# The light points are the original data. The dark points are the projected ones.
# This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed,
# leaving only the components of the data with the highest variance.

#?????????????????????
MoleQM_X_new = pca.inverse_transform(MoleQM_X_PCA)

fig = plt.figure(figsize=(16,6))
ax  = fig.add_subplot(111)
scatter = ax.scatter(MoleQM_X[:,0], MoleQM_X[:,1], c=MoleQM_Y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=20)
colorbar = fig.colorbar(scatter, ax=ax, label = "Atomization Energy (kcal/mol) ")

fig = plt.figure(figsize=(16,6))
ax  = fig.add_subplot(111)
scatter = ax.scatter(MoleQM_X_new[:,0], MoleQM_X_new[:,1], c=MoleQM_Y, s=45, edgecolors='green', cmap=cm.jet_r, alpha=20)
colorbar = fig.colorbar(scatter, ax=ax, label = "Atomization Energy (kcal/mol) ")

plt.show()


# To understand the effect of the PCA reduction, I performed an inverse transform of the reduced data and plot it along with the original data.
# The light points are the original data. The dark points are the projected ones.
# This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed,
# leaving only the components of the data with the highest variance.

"""#### PCA using Eigenvalues

####**eigenspectrum**: Only the eigenvalues of the matrix are returned sorted by their absolute value in descending order. On one hand, it is a more compact descriptor, but on the other hand, it potentially loses information encoded in the CM interactions.
"""

from google.colab import files
uploaded = files.upload()

from google.colab import drive
drive.mount('/content/gdrive')

#Eigenval = pd.read_csv('coulombL.csv.webarchive')

coulomb = pd.read_csv(
    'https://raw.githubusercontent.com/bhimmetoglu/RoboBohr/master/data/coulombL.csv', index_col=0)
print(coulomb)


# We get 50 colums when data is represented by Î» (the eigenspectrum ). 50 dimensional vector of eigenvalues

"""###PCA"""

# Standaraizing the coulomb data.

standard_coulomb = StandardScaler().fit_transform(coulomb)

pca = PCA(n_components=2)
coulomb_pca = pca.fit_transform(coulomb)
PCA_score_eigen = pca.explained_variance_ratio_
V = pca.components_

print(pca.explained_variance_ratio_)
# shows the percentage of variance that is attributed by each of the selected components

# Now I want to see what the dimensions of the coulomb_pca object are.
coulomb_pca.shape

# why 16272 instead of 16242 This is 30 extra rows??
#The principal components are constructed from the eigenvectors of the covariance matrix, and are ordered with increasing eigenvalues

# Visualizing PCA components
#I check that the inputs are reasonably distributed by plotting a two-dimensional PCA of data.

fig = plt.figure(figsize=(16, 6))
ax1 = fig.add_subplot(111)
ax1.set_ylim([0,1])
lin1 = ax1.scatter(range(0, int(PCA_score_eigen.shape[0])), PCA_score_eigen, c = 'b', label = 'no random noise')

plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Components Pattern')

plt.show()

# Amoung the 50 principal components, the first two account for 32% of the variability in the data

fig = plt.figure(figsize=(16, 6))
axis2 = fig.add_subplot(111)
axis2.scatter(coulomb_pca[:,0], coulomb_pca[:,1])

plt.xlabel('Coulomb PC1')
plt.ylabel('Coulomb PC2')
plt.title('Coulomb data with PCA')

plt.show()

# illustrates Eps (enertgy of pseudo-potential)as a function of the first two principal components (Z1 and Z2), which display a peculiar nonlinear dependence on the two features.

# Something is not working properly.  I'm going to try to just copy and paste
# the code from the website with the example and change as little as possible,
# and see if I can get the right graph.

# data taken from https://github.com/bhimmetoglu/RoboBohr/tree/master/data
#columbl = pd.read_csv('/content/gdrive/MyDrive/coulombL.csv', header=None, index_col=0)

#from sklearn.decomposition import PCA

#ca = PCA(n_components=2)
#z = pca.fit_transform(columbl)
#z = pd.DataFrame(z)

MoleQM_Y.shape

"""## Modeling using the full Coulumb Matrices as features

### Splitting the Data
"""

from sklearn.model_selection import train_test_split

MoleQM_X_train_PCA, MoleQM_X_test_PCA, MoleQM_Y_train, MoleQM_Y_test = train_test_split(MoleQM_X_PCA, MoleQM_Y, test_size=0.3)

#70 % of the data is randomly selected as the training set, and 30 % as the test set

print(MoleQM_X_train_PCA.shape)
print(MoleQM_X_test_PCA.shape)
print(MoleQM_Y_train.shape)
print(MoleQM_Y_test.shape)

#Apply the mapping (transform) to both the training set and the test set.????

#MoleQM_X_train = pca.transform(MoleQM_X_train)
#MoleQM_X_test = pca.transform(MoleQM_X_test)

from sklearn.metrics import mean_squared_error

"""look at tis https://towardsdatascience.com/machine-learning-general-process-8f1b510bd8af"""

from sklearn import model_selection
from sklearn.linear_model import Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import KFold
import random


# Models
num_folds = 5 #This is the number number of folds in which I split the data
scoring = "neg_mean_squared_error"
#seed = 12

models = []
models.append(('KNN', KNeighborsRegressor()))
models.append(('LASSO', Lasso()))
models.append(('RIDGE', Ridge()))

results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=num_folds, random_state=None) # K-Folds cross-validator. Split dataset into k consecutive folds (without shuffling by default).
    #Each fold is then used once as a validation while the k - 1 remaining folds form the training set. We have 10 folds here.


    Scores = model_selection.cross_val_score(model, MoleQM_X_train_PCA, MoleQM_Y_train, cv=kfold, scoring=scoring) # Cross Validation.
    results.append(cv_results)
    names.append(name)
    Results = "%s: %f (%f)" % (name, Scores.mean(),   Scores.std())
    print(Results)

# Gives a list of each algorithm short name, the mean accuracy and the standard deviation accuracy. QUESTION:

# boxplot algorithm comparison
fig = plt.figure()
fig.suptitle('Model Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

"""####KNeighbors"""

from sklearn.metrics import mean_squared_error

# QUESTION: why different scores?? Is it because this. is donee with split data? If so, should I split data or do model PCA witout splitting?


from sklearn.neighbors import KNeighborsRegressor

KN_model = KNeighborsRegressor(weights='distance')
KN_model.fit(MoleQM_X_train_PCA , MoleQM_Y_train)
predict = KN_model.predict(MoleQM_X_train_PCA)
mse =mean_squared_error(MoleQM_Y_train, predict)
print(np.sqrt(mse))

#score = KNeighborsRegressor.score(MoleQM_Y_train, MoleQM_Y_test)
#print(score)

plt.figure()
sns.distplot(predict, kde=True, color='r')
plt.xlabel('Data')
plt.ylabel('Pred')
plt.title('K-Neighbors Distrubtion')

# Thgese comments are for the unsplitted data. applied to this model
plt.figure()
sns.distplot(predict2, kde=True, color='r')
plt.xlabel('Data')
plt.ylabel('Pred')
plt.title('K-Neighbors Distrubtion')

# Looks like KNN does a pretty good job of predicting the correct frequencies of
# the different energies.

# Also, we see that the mean squared error is MSE = 6.839212048013726e-09,
# which is pretty great.

#Ridge with unsplitted and no PCA data

from sklearn.linear_model import Ridge, Lasso

ridge_model = Ridge()
ridge_model.fit(MoleQM_X, MoleQM_Y)
predict3 = ridge_model.predict(MoleQM_X)
mse3 = mean_squared_error(MoleQM_Y, predict3)
print (np.sqrt(mse3))

plt.figure()
sns.distplot(predict3, kde=True, color='g')
plt.xlabel('Data')
plt.ylabel('Pred')
plt.title("RIDGE Distrubtion")

# Looks like ridge regression also does a pretty good job, but perhaps not as
# good as KNN.

# Also, we see that the mean squared error is MSE = 0.7114090821358027,
# which isn't as good as KNN.

#Lasso

lasso_model = Lasso()
lasso_model.fit(MoleQM_X, MoleQM_Y)
predict4 = lasso_model.predict(MoleQM_X)
mse4 = mean_squared_error(MoleQM_Y, predict4)
print (np.sqrt(mse4))

plt.figure()
sns.distplot(predict4, kde=True, color='b')
plt.xlabel('Data')
plt.ylabel('Pred')
plt.title("LASSO Distribution")

# Looks like LASSO does a pretty poor job of predicting the frequencies of the
# energies.  You can hardly tell that this is a normal distribution.

# Also, we see that the mean-squared error is MSE = 1.5238473811102473,
# which is the worst of the three.

"""#Conclusion"""

PCA is one of the most thought after techniques for dimensionality reduction of high dimensional data.  It can reduce a large number of data points to just a few in a matter of seconds. Applying this powerful technique to our highly dimensional data helped confirm the latter statement. In my work, I used two sets of data describing an intrinsic relationship between properties sharing a particular property and their ground energies. The following findings illustrate the importance of applying this technique.

The original data was sorted using a designed matrix (Coulumb Matrix) which describes a relationship between atoms composing a particular range of. molecules. PCA performed bettwr on. the unrolled data (the Robohbr. data)

The plots of the 2 main PCa components, suggest a global and local statistics with highly non-Gaussian distribution.


The models show that only KNN model can benefit from using PCA as a dimensionality reduction technique which is not the case for Lasso and Ridge